{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34fd4e2d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Análise de Histórico de Crédito"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e65e774",
   "metadata": {},
   "source": [
    "## Setup do Ambiente e Bibliotecas\n",
    "\n",
    "Nesta célula, configuramos o ambiente, importando todas as bibliotecas e módulos customizados necessários."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e122090c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import os\n",
    "from dataclasses import asdict\n",
    "import logging\n",
    "\n",
    "# --- Imports dos Módulos do Projeto ---\n",
    "from data_parser import HistoricoCreditoXMLParser\n",
    "from data_class import EnvioHistoricoCredito\n",
    "from example import xml_example\n",
    "from bigquery_uploader import BigQueryUploader\n",
    "\n",
    "# --- Configuração de Logging ---\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e606ae55",
   "metadata": {},
   "source": [
    "## 1) Acesso e Leitura dos Dados em Formato XML\n",
    "\n",
    "A primeira etapa consiste em desenvolver uma solução para ler os dados do arquivo XML original. A abordagem escolhida foi a criação de um parser robusto em Python, utilizando a biblioteca padrão `xml.etree.ElementTree` para processar a estrutura hierárquica do XML.\n",
    "\n",
    "### Arquitetura da Solução\n",
    "\n",
    "1.  **`data_class.py`**: Um módulo contendo `dataclasses` que espelham a estrutura do XML. Esta abordagem oferece tipagem estática, validação implícita de campos e uma forma limpa e orientada a objetos de representar os dados em memória.\n",
    "2.  **`data_parser.py`**: Contém a classe `HistoricoCreditoXMLParser`, responsável por mapear cada tag e atributo para as `dataclasses` correspondentes, tratando tipos de dados e campos opcionais.\n",
    "\n",
    "### Premissas Utilizadas\n",
    "\n",
    "*   **Estrutura do XML**: O arquivo XML é bem-formado e segue o schema definido no dicionário de dados.\n",
    "*   **Tipos de Dados**: Datas são tratadas como strings, valores monetários como `float` e quantidades como `int`.\n",
    "*   **Tratamento de Erros**: O parser é resiliente a campos opcionais ausentes e registra *warnings* em caso de erros de conversão de tipo, sem interromper a execução.\n",
    "\n",
    "### Aplicação em Prática\n",
    "\n",
    "Para demonstrar o funcionamento, utilizamos a função `xml_example()` para gerar um arquivo XML completo. Em seguida, o parser é utilizado para lê-lo e carregar os dados em um objeto Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f614408",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# --- 1. Geração do Arquivo de Exemplo ---\n",
    "xml_file_path = xml_example()\n",
    "logging.info(f\"Arquivo XML de exemplo gerado: '{xml_file_path}'\")\n",
    "\n",
    "# --- 2. Instanciação e Uso do Parser ---\n",
    "parser = HistoricoCreditoXMLParser()\n",
    "dados_credito: EnvioHistoricoCredito = parser.parse_xml_file(xml_file_path)\n",
    "\n",
    "# --- 3. Demonstração do Acesso aos Dados Parseados ---\n",
    "print(\"\\n--- Exemplo de Acesso aos Dados Parseados ---\")\n",
    "for cliente in dados_credito.Cli:\n",
    "    print(f\"\\nCliente: {cliente.nm_cli} (ID: {cliente.idfc_cli})\")\n",
    "    if cliente.operacoes:\n",
    "        print(f\"  Encontradas {len(cliente.operacoes)} operações.\")\n",
    "        for operacao in cliente.operacoes:\n",
    "            valor_contratado = \"N/A\"\n",
    "            if operacao.det_opr and operacao.det_opr.vl_obgc is not None:\n",
    "                valor_contratado = f\"R$ {operacao.det_opr.vl_obgc:,.2f}\"\n",
    "            \n",
    "            print(f\"  -> Contrato: {operacao.nr_ctr} | Modalidade: {operacao.cd_mdld} | Valor: {valor_contratado}\")\n",
    "    else:\n",
    "        print(\"  -> Nenhuma operação registrada para este cliente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe3761b",
   "metadata": {},
   "source": [
    "## 2) Armazenamento em Formato Analítico\n",
    "\n",
    "Com os dados em memória, o próximo passo é armazená-los em um formato otimizado para análises. A solução principal proposta utiliza o **Google BigQuery**, mas uma abordagem relacional tradicional também é uma alternativa viável.\n",
    "\n",
    "### Solução Principal: Modelo Desnormalizado (Google BigQuery)\n",
    "\n",
    "1.  **Plataforma: Google BigQuery**: Solução *serverless* e escalável, ideal para queries analíticas (OLAP). Seu suporte nativo a dados aninhados (`STRUCT`/`RECORD`) é perfeito para representar a estrutura do XML sem perda de informação e sem a necessidade de `JOINs` complexos.\n",
    "\n",
    "2.  **Formato de Ingestão: NDJSON (Newline Delimited JSON)**: Formato padrão para ingestão em batch no BigQuery. É gerado facilmente a partir dos nossos objetos e permite parsing paralelo eficiente pela plataforma.\n",
    "\n",
    "3.  **Modelagem: Tabela Única com Campos Aninhados**: Mantemos a hierarquia original em vez de achatar os dados. Isso melhora a performance de consultas (`UNNEST` é mais otimizado que `JOINs` em larga escala) e torna o schema mais intuitivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d940ae5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparar_para_bigquery(dados: EnvioHistoricoCredito, output_filename: str) -> str:\n",
    "    \"\"\"Converte o objeto dataclass para um dicionário e o salva em formato NDJSON.\"\"\"\n",
    "    dados_dict = asdict(dados)\n",
    "    \n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        json_record = json.dumps(dados_dict, ensure_ascii=False)\n",
    "        f.write(json_record + '\\n')\n",
    "        \n",
    "    logging.info(f\"Arquivo '{output_filename}' pronto para ingestão no BigQuery.\")\n",
    "    return output_filename\n",
    "\n",
    "# --- PREPARAÇÃO E UPLOAD PARA BIGQUERY ---\n",
    "\n",
    "# 1. Preparar o arquivo NDJSON\n",
    "ndjson_file = preparar_para_bigquery(dados_credito, \"historico_credito_upload.ndjson\")\n",
    "print(f\"Arquivo NDJSON gerado: {ndjson_file}\")\n",
    "\n",
    "# 2. Configurar credenciais e detalhes do BigQuery\n",
    "GOOGLE_PROJECT_ID = \"quod_dwh\"\n",
    "BQ_DATASET_ID = \"credit_catalog\"\n",
    "BQ_TABLE_ID = \"historico_credito_final\"\n",
    "\n",
    "# 3. Executar o upload\n",
    "try:\n",
    "    uploader = BigQueryUploader(project_id=GOOGLE_PROJECT_ID, dataset_id=BQ_DATASET_ID)\n",
    "    print(f\"Verificando/Criando tabela '{BQ_TABLE_ID}'...\")\n",
    "    uploader.create_or_update_table(BQ_TABLE_ID)\n",
    "    print(f\"Iniciando upload do arquivo '{ndjson_file}'...\")\n",
    "    uploader.upload_ndjson_file(BQ_TABLE_ID, ndjson_file)\n",
    "    print(f\"\\nProcesso concluído! Dados carregados em: `{GOOGLE_PROJECT_ID}.{BQ_DATASET_ID}.{BQ_TABLE_ID}`\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nOcorreu um erro durante o upload: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca043c3",
   "metadata": {},
   "source": [
    "### Alternativa: Modelo Relacional Normalizado (SQL Server / PostgreSQL)\n",
    "\n",
    "Uma abordagem alternativa seria usar um banco de dados relacional tradicional. Nesse cenário, \"achatamos\" a estrutura hierárquica do XML, distribuindo os dados em múltiplas tabelas normalizadas. Esta modelagem, conhecida como **Esquema Estrela (Star Schema)**, é clássica em Data Warehousing.\n",
    "\n",
    "#### Modelo de Dados Relacional\n",
    "\n",
    "O processo de ETL (Extração, Transformação e Carga) seria mais complexo, pois precisaria popular diversas tabelas e manter a integridade referencial através de chaves primárias e estrangeiras. Índices seriam criados nas chaves estrangeiras (FK) e em colunas frequentemente usadas em filtros (`WHERE`) para otimizar a performance dos `JOINs`.\n",
    "\n",
    "Abaixo, uma ilustração de como o modelo ficaria:\n",
    "\n",
    "```mermaid\n",
    "erDiagram\n",
    "    DIM_CLIENTE {\n",
    "        int id_cliente PK \"Índice Clustered\"\n",
    "        varchar idfc_cli \"Índice Non-Clustered\"\n",
    "        varchar nm_cli\n",
    "        varchar tip_cli\n",
    "        varchar ntz_rlc\n",
    "    }\n",
    "\n",
    "    FACT_OPERACAO {\n",
    "        int id_operacao PK \"Índice Clustered\"\n",
    "        int id_cliente FK\n",
    "        varchar nr_ctr \"Índice Non-Clustered\"\n",
    "        varchar cd_mdld\n",
    "        date dt_ctrc\n",
    "        decimal vl_obgc\n",
    "        decimal sdo_dvdr\n",
    "        int qt_pcl\n",
    "    }\n",
    "\n",
    "    FACT_PARCELA {\n",
    "        int id_parcela PK\n",
    "        int id_operacao FK\n",
    "        date dt_vnct\n",
    "        decimal vl_pcl\n",
    "        varchar tipo_parcela \"Anterior, Futura, etc.\"\n",
    "    }\n",
    "    \n",
    "    FACT_PAGAMENTO {\n",
    "        int id_pagamento PK\n",
    "        int id_parcela FK\n",
    "        date dt_pgto\n",
    "        decimal vl_pgto\n",
    "    }\n",
    "\n",
    "    DIM_CLIENTE ||--o{ FACT_OPERACAO : \"possui\"\n",
    "    FACT_OPERACAO ||--o{ FACT_PARCELA : \"contém\"\n",
    "    FACT_PARCELA ||--o{ FACT_PAGAMENTO : \"recebe\"\n",
    "```\n",
    "\n",
    "#### Vantagens e Desvantagens\n",
    "\n",
    "*   **Vantagens**:\n",
    "    *   **Integridade de Dados**: Fortemente garantida por chaves estrangeiras.\n",
    "    *   **Redução de Redundância**: A normalização evita a repetição de dados.\n",
    "    *   **Familiaridade**: Modelo amplamente conhecido por analistas e desenvolvedores SQL.\n",
    "*   **Desvantagens**:\n",
    "    *   **Complexidade nas Consultas**: Requer múltiplos `JOINs` para reconstruir a visão completa, o que pode ser menos performático para análises complexas em grandes volumes.\n",
    "    *   **ETL Mais Complexo**: O processo de carga precisa coordenar a inserção em várias tabelas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5114409b",
   "metadata": {},
   "source": [
    "## 3) Melhorias na Documentação\n",
    "\n",
    "Uma boa documentação é crucial para a governança e utilização correta dos dados. O dicionário de dados poderia ser enriquecido com as seguintes informações:\n",
    "\n",
    "1.  **Tipagem e Formato Explícito**: Especificar o tipo de dado esperado para cada campo (ex: `DtCtrc`: `DATE(YYYY-MM-DD)`, `VlObgc`: `DECIMAL(18, 2)`).\n",
    "\n",
    "2.  **Descrição de Domínios**: Para campos de código (`CdMdld`, `TipCli`), fornecer uma tabela \"de-para\" com o significado de cada valor (ex: `CdMdld`: `001` - Crédito Pessoal).\n",
    "\n",
    "3.  **Exemplos Práticos**: Incluir um valor de exemplo para cada campo para elucidar o formato esperado (ex: `NrCtr`: \"Ex: `CTA-789123-01`\").\n",
    "\n",
    "4.  **Regras de Negócio**: Explicitar as regras que governam o preenchimento (ex: \"O campo `DtContCtCsr` só é preenchido se `CdMdld` for Consórcio\").\n",
    "\n",
    "5.  **Significado dos Comandos**: Documentar o que cada valor para os campos `Cmdo*` significa (ex: `I` - Inclusão, `A` - Alteração, `E` - Exclusão).\n",
    "\n",
    "6.  **Metadados Gerais**: Incluir controle de versão do layout, origem dos dados, frequência de atualização e contatos responsáveis (Data Stewards)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08af75ea",
   "metadata": {},
   "source": [
    "## 4) Consultas SQL para Análise de Dados\n",
    "\n",
    "Com os dados estruturados, podemos realizar consultas analíticas. A sintaxe varia conforme a abordagem escolhida.\n",
    "\n",
    "**Nota**: Nos exemplos, substitua os nomes das tabelas pelos caminhos corretos do seu ambiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842cb4ce",
   "metadata": {},
   "source": [
    "### Consulta 4a: Contratos e Soma de Valor por Modalidade\n",
    "\n",
    "*Lista todos os contratos únicos por CPF, sua modalidade, valor contratado e uma coluna adicional com a soma total contratada por aquele cliente naquela modalidade.*\n",
    "\n",
    "#### Abordagem BigQuery (Desnormalizada)\n",
    "Utiliza `UNNEST` para acessar os dados aninhados de forma eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f497f7",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Sintaxe para Google BigQuery\n",
    "SELECT\n",
    "  cliente.idfc_cli AS cpf_cnpj_cliente,\n",
    "  operacao.nr_ctr AS numero_contrato,\n",
    "  operacao.cd_mdld AS modalidade_contrato,\n",
    "  COALESCE(operacao.det_opr.vl_obgc, 0) AS valor_contratado,\n",
    "  SUM(COALESCE(operacao.det_opr.vl_obgc, 0)) OVER (PARTITION BY cliente.idfc_cli, operacao.cd_mdld) AS soma_valor_por_modalidade_cliente\n",
    "FROM\n",
    "  `quod_dwh.credit_catalog.historico_credito_final`,\n",
    "  UNNEST(Cli) AS cliente,\n",
    "  UNNEST(cliente.operacoes) AS operacao\n",
    "WHERE\n",
    "  operacao.nr_ctr IS NOT NULL\n",
    "ORDER BY\n",
    "  cpf_cnpj_cliente,\n",
    "  modalidade_contrato;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb575dad",
   "metadata": {},
   "source": [
    "#### Abordagem Relacional (SQL Server/PostgreSQL)\n",
    "\n",
    "Requer `JOIN` entre as tabelas `DIM_CLIENTE` e `FACT_OPERACAO`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bea99f",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Sintaxe para SQL Server / PostgreSQL\n",
    "WITH OperacoesCliente AS (\n",
    "    SELECT\n",
    "        c.idfc_cli,\n",
    "        o.nr_ctr,\n",
    "        o.cd_mdld,\n",
    "        o.vl_obgc AS valor_contratado\n",
    "    FROM\n",
    "        DIM_CLIENTE c\n",
    "    JOIN\n",
    "        FACT_OPERACAO o ON c.id_cliente = o.id_cliente\n",
    ")\n",
    "SELECT\n",
    "    idfc_cli AS cpf_cnpj_cliente,\n",
    "    nr_ctr AS numero_contrato,\n",
    "    cd_mdld AS modalidade_contrato,\n",
    "    valor_contratado,\n",
    "    SUM(valor_contratado) OVER (PARTITION BY idfc_cli, cd_mdld) AS soma_valor_por_modalidade_cliente\n",
    "FROM\n",
    "    OperacoesCliente\n",
    "ORDER BY\n",
    "    cpf_cnpj_cliente,\n",
    "    modalidade_contrato;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81626af",
   "metadata": {},
   "source": [
    "### Consulta 4b: Indicador de Parcelas em Atraso\n",
    "\n",
    "*Para cada contrato de cada cliente, gera um indicador booleano (`TRUE`/`FALSE`) que aponta se existe ao menos uma parcela anterior não quitada (em atraso).*\n",
    "\n",
    "#### Abordagem BigQuery (Desnormalizada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08712e2c",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Sintaxe para Google BigQuery\n",
    "WITH ContratosComStatusDeAtraso AS (\n",
    "  SELECT\n",
    "    cliente.idfc_cli,\n",
    "    operacao.nr_ctr,\n",
    "    (\n",
    "      SELECT LOGICAL_OR(p.dt_vnct_pcl_ant < CURRENT_DATE() AND NOT EXISTS (SELECT 1 FROM UNNEST(p.pgto_pcl_ant)))\n",
    "      FROM UNNEST(operacao.pcl_ant) AS p\n",
    "    ) AS tem_atraso_normal,\n",
    "    (\n",
    "      SELECT LOGICAL_OR(p_csr.dt_vnct_pcl_ant_csr < CURRENT_DATE() AND NOT EXISTS (SELECT 1 FROM UNNEST(p_csr.pgto_pcl_ant_csr)))\n",
    "      FROM UNNEST(operacao.pcl_ant_csr) AS p_csr\n",
    "    ) AS tem_atraso_consorcio\n",
    "  FROM\n",
    "    `quod_dwh.credit_catalog.historico_credito_final`,\n",
    "    UNNEST(Cli) AS cliente,\n",
    "    UNNEST(cliente.operacoes) AS operacao\n",
    ")\n",
    "SELECT\n",
    "  idfc_cli AS cpf_cnpj_cliente,\n",
    "  nr_ctr AS numero_contrato,\n",
    "  COALESCE(tem_atraso_normal, FALSE) OR COALESCE(tem_atraso_consorcio, FALSE) AS indicador_possui_atraso\n",
    "FROM\n",
    "  ContratosComStatusDeAtraso\n",
    "WHERE\n",
    "  nr_ctr IS NOT NULL\n",
    "ORDER BY\n",
    "  cpf_cnpj_cliente,\n",
    "  numero_contrato;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87707978",
   "metadata": {},
   "source": [
    "#### Abordagem Relacional (SQL Server/PostgreSQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffcb17e",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Sintaxe para SQL Server / PostgreSQL\n",
    "SELECT\n",
    "    c.idfc_cli AS cpf_cnpj_cliente,\n",
    "    o.nr_ctr AS numero_contrato,\n",
    "    -- Verifica se existe alguma parcela para a operação que esteja em atraso\n",
    "    CAST(CASE \n",
    "        WHEN EXISTS (\n",
    "            SELECT 1\n",
    "            FROM FACT_PARCELA par\n",
    "            WHERE \n",
    "                par.id_operacao = o.id_operacao\n",
    "                AND par.dt_vnct < GETDATE() -- ou CURRENT_DATE para PostgreSQL\n",
    "                AND NOT EXISTS (\n",
    "                    SELECT 1 \n",
    "                    FROM FACT_PAGAMENTO pag \n",
    "                    WHERE pag.id_parcela = par.id_parcela\n",
    "                )\n",
    "        ) THEN 1 \n",
    "        ELSE 0 \n",
    "    END AS bit) AS indicador_possui_atraso\n",
    "FROM\n",
    "    DIM_CLIENTE c\n",
    "JOIN\n",
    "    FACT_OPERACAO o ON c.id_cliente = o.id_cliente\n",
    "ORDER BY\n",
    "    c.idfc_cli,\n",
    "    o.nr_ctr;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5f8a79",
   "metadata": {},
   "source": [
    "# Parte 2: Modelagem Estatística\n",
    "\n",
    "Beleza, agora que já demos uma olhada nos dados na primeira parte, vamos para o que interessa: construir um modelo preditivo. A ideia aqui é criar um sistema que aprenda com os dados e consiga prever se um novo cliente pertence à \"Classe 0\" ou à \"Classe 1\".\n",
    "\n",
    "Para fazer isso de forma organizada, vamos seguir um plano:\n",
    "\n",
    "1.  **Preparar o Terreno (Engenharia de Features)**: Antes de mais nada, vamos fazer uma \"faxina\" nos dados, lidando com informações faltando e selecionando as variáveis que parecem mais promissoras.\n",
    "2.  **A Competição de Modelos**: Vamos colocar diferentes tipos de algoritmos para competir e ver qual deles se adapta melhor aos nossos dados.\n",
    "3.  **Ajuste Fino (Tuning)**: Depois de escolher um campeão, vamos fazer uns ajustes finos nos \"botões\" do modelo para extrair sua melhor performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8149d68",
   "metadata": {},
   "source": [
    "## Setup do Ambiente\n",
    "\n",
    "Primeiro, a gente carrega todas as \"ferramentas\" (bibliotecas Python) que vamos usar. É como um chef de cozinha que organiza todos os ingredientes e utensílios antes de começar a cozinhar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed0202b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report, confusion_matrix\n",
    "import xgboost as xgb\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac52684f",
   "metadata": {},
   "source": [
    "## 1) Preparação e Seleção de Features\n",
    "\n",
    "O primeiro passo na modelagem é sempre arrumar a casa. Nossos dados brutos precisam de um trato antes de poderem ser usados.\n",
    "\n",
    "O que vamos fazer aqui é:\n",
    "- **Limpar o Lixo**: Variáveis com muitos dados faltando (mais de 70%) geralmente mais atrapalham do que ajudam. Vamos removê-las, junto com colunas de identificação como `id` e `safra`.\n",
    "- **Tapar os Buracos**: Nos dados que sobraram, ainda existem alguns valores faltando. Vamos preenchê-los usando a mediana, que é uma forma segura de fazer isso sem ser muito afetado por valores extremos.\n",
    "- **Escolher os Melhores Jogadores**: Mesmo depois da limpeza, ainda temos muitas variáveis. Para não confundir nosso modelo, vamos usar uma técnica chamada **RFE** para nos ajudar a escolher as **50 variáveis** que parecem ter mais \"poder de fogo\" para prever o resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528e3c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_csv(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        cleaned_lines = [line.strip().strip('\"') for line in lines]\n",
    "        csv_string = \"\\n\".join(cleaned_lines)\n",
    "        csv_io = io.StringIO(csv_string)\n",
    "        df = pd.read_csv(csv_io)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "df = load_and_clean_csv('Anexo 2 - Base Modelagem.csv')\n",
    "\n",
    "if not df.empty:\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    df[numeric_cols] = df[numeric_cols].mask(df[numeric_cols] < 0, np.nan)\n",
    "\n",
    "    missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "    high_missing_vars = missing_percentage[missing_percentage > 70]\n",
    "    \n",
    "    features_to_drop = ['id', 'safra'] + high_missing_vars.index.tolist()\n",
    "    df_cleaned = df.drop(columns=features_to_drop, errors='ignore')\n",
    "    \n",
    "    X = df_cleaned.drop(columns='resposta')\n",
    "    y = df_cleaned['resposta']\n",
    "\n",
    "    preprocessing_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    X_processed = preprocessing_pipeline.fit_transform(X, y)\n",
    "    X_processed = pd.DataFrame(X_processed, columns=X.columns)\n",
    "    \n",
    "    NUM_FEATURES_TO_SELECT = 50\n",
    "    estimator = LogisticRegression(solver='liblinear', class_weight='balanced')\n",
    "    selector = RFE(estimator, n_features_to_select=NUM_FEATURES_TO_SELECT, step=10)\n",
    "    \n",
    "    selector = selector.fit(X_processed, y)\n",
    "    \n",
    "    selected_features = X.columns[selector.support_].tolist()\n",
    "    X_final = X_processed[selected_features]\n",
    "    print(f\"{len(selected_features)} features selecionadas e prontas para a modelagem.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46d9edc",
   "metadata": {},
   "source": [
    "## 2) Construção e Comparação de Modelos\n",
    "\n",
    "Com nossos 50 melhores \"ingredientes\" selecionados, é hora de cozinhar! Vamos testar algumas \"receitas\" (modelos) diferentes para ver qual delas produz o melhor resultado.\n",
    "\n",
    "Nossos competidores são:\n",
    "- **Logistic Ridge**: Um modelo clássico, rápido e confiável. É como o \"arroz com feijão\" da modelagem, mas muitas vezes funciona surpreendentemente bem.\n",
    "- **Random Forest**: Um modelo mais complexo que usa várias \"árvores de decisão\" para chegar a um consenso. É ótimo para pegar relações mais complicadas nos dados.\n",
    "- **XGBoost**: O \"campeão de competições\". É um algoritmo super poderoso, famoso por conseguir ótimos resultados em dados como os nossos.\n",
    "\n",
    "Para ter certeza de que a competição é justa, vamos usar uma técnica chamada **Validação Cruzada**. Em vez de testar cada modelo só uma vez, vamos testá-los 5 vezes em diferentes \"fatias\" dos dados e tirar a média. Isso nos dá uma ideia muito mais robusta de quem é o verdadeiro vencedor. A nota que daremos a cada um é a **AUC-ROC**, uma métrica que mede o quão bom o modelo é em separar os \"bons\" dos \"maus\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898b0a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_final' in locals():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_final, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    scale_pos_weight_value = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "    models = {\n",
    "        \"Logistic Ridge\": Pipeline([('model', LogisticRegression(penalty='l2', solver='liblinear', class_weight='balanced', random_state=42))]),\n",
    "        \"Random Forest\": Pipeline([('model', RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42, n_jobs=-1))]),\n",
    "        \"XGBoost\": Pipeline([('model', xgb.XGBClassifier(scale_pos_weight=scale_pos_weight_value, use_label_encoder=False, eval_metric='logloss', random_state=42))])\n",
    "    }\n",
    "    \n",
    "    cv_results = {}\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    print(\"\\n--- Resultados da Validação Cruzada (AUC-ROC no Treino) ---\")\n",
    "    for name, model in models.items():\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=kfold, scoring='roc_auc', n_jobs=-1)\n",
    "        cv_results[name] = scores\n",
    "        print(f\"-> {name}: Média AUC = {scores.mean():.4f} (Desvio Padrão = {scores.std():.4f})\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sorted_results = sorted(cv_results.items(), key=lambda item: item[1].mean())\n",
    "    sorted_labels = [item[0] for item in sorted_results]\n",
    "    sorted_scores = [item[1] for item in sorted_results]\n",
    "    plt.boxplot(sorted_scores, labels=sorted_labels, vert=False)\n",
    "    plt.title('Comparação de Modelos via Validação Cruzada')\n",
    "    plt.xlabel('AUC-ROC')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c60dec",
   "metadata": {},
   "source": [
    "### Análise da Competição\n",
    "\n",
    "Olhando o gráfico acima, fica claro que o **Logistic Ridge** foi o grande vencedor da nossa competição. Ele não só teve a maior média de AUC-ROC, como também foi bem consistente nos testes (a \"caixinha\" dele é pequena e está mais à direita). Isso é bem interessante, porque mostra que, para este problema, um modelo mais simples e linear conseguiu extrair mais informações úteis do que os algoritmos mais complexos como Random Forest e XGBoost.\n",
    "\n",
    "Agora que temos nosso campeão, vamos para a última etapa: o ajuste fino."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c1b8ac",
   "metadata": {},
   "source": [
    "## 3) Otimização de Hiperparâmetros (Tuning)\n",
    "\n",
    "Já sabemos que o Logistic Ridge é o melhor modelo para o nosso caso. Agora, vamos fazer um \"tuning\" nele. É como se a gente já tivesse escolhido o melhor carro de corrida, e agora vamos ajustar os \"botões\" e \"alavancas\" (os hiperparâmetros) dele para ver se conseguimos um desempenho ainda melhor.\n",
    "\n",
    "Vamos focar no parâmetro `C`, que controla a \"força\" da regularização do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1de69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cv_results' in locals():\n",
    "    best_model_name = \"Logistic Ridge\"\n",
    "    print(f\"\\nModelo selecionado para tuning: {best_model_name}\")\n",
    "    \n",
    "    param_dist_lr = { 'model__C': np.logspace(-3, 3, 20) }\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        models[best_model_name],\n",
    "        param_distributions=param_dist_lr,\n",
    "        n_iter=15, scoring='roc_auc', cv=kfold,\n",
    "        n_jobs=-1, random_state=42, verbose=0\n",
    "    )\n",
    "    \n",
    "    print(\"Iniciando a busca de hiperparâmetros...\")\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"\\nMelhores parâmetros encontrados: {random_search.best_params_}\")\n",
    "    print(f\"Melhor AUC na validação cruzada após tuning: {random_search.best_score_:.4f}\")\n",
    "    \n",
    "    best_tuned_model = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daec56d8",
   "metadata": {},
   "source": [
    "### Avaliação Final do Modelo Otimizado\n",
    "\n",
    "Chegou a hora da verdade. Vamos pegar nosso modelo campeão, o **Logistic Ridge**, já com os melhores ajustes, e colocá-lo para a prova final: o conjunto de teste. Esses são dados que o modelo nunca viu antes, então o resultado aqui é a nossa estimativa mais honesta de como ele se sairia no mundo real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ff046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'best_tuned_model' in locals():\n",
    "    best_model_name = \"Logistic Ridge\"\n",
    "    print(f\"\\n--- Avaliação Final do Modelo Otimizado ({best_model_name}) no Conjunto de Teste ---\")\n",
    "    \n",
    "    y_pred_proba = best_tuned_model.predict_proba(X_test)[:, 1]\n",
    "    y_pred_class = best_tuned_model.predict(X_test)\n",
    "\n",
    "    test_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    print(f\"AUC-ROC no conjunto de teste: {test_auc:.4f}\")\n",
    "    print(\"\\nRelatório de Classificação no Teste:\")\n",
    "    print(classification_report(y_test, y_pred_class, target_names=['Classe 0', 'Classe 1']))\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred_class)\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Classe 0', 'Classe 1'], yticklabels=['Classe 0', 'Classe 1'])\n",
    "    plt.title(f'Matriz de Confusão - Modelo Otimizado ({best_model_name})')\n",
    "    plt.ylabel('Verdadeiro')\n",
    "    plt.xlabel('Predito')\n",
    "    plt.show()\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Curva ROC (área = {test_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.title(f'Curva ROC - Modelo Otimizado ({best_model_name})')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398b379e",
   "metadata": {},
   "source": [
    "### Conclusão Final\n",
    "\n",
    "Bom, chegamos ao fim da nossa jornada de modelagem!\n",
    "\n",
    "O nosso modelo final, um **Logistic Ridge** com os parâmetros otimizados, alcançou uma **AUC-ROC de 0.6635** no teste final. Isso significa que ele tem um poder de discriminação razoável, sendo consideravelmente melhor do que uma escolha aleatória.\n",
    "\n",
    "Olhando o \"Relatório de Classificação\", vemos algo bem interessante:\n",
    "- **Recall (Classe 1) de 0.57**: Isso é ótimo! Quer dizer que, de todos os clientes que realmente são da \"Classe 1\" (os que mais nos interessam), nosso modelo conseguiu **encontrar 57%** deles.\n",
    "- **Precisão (Classe 1) de 0.14**: Aqui está o ponto de atenção. Dos clientes que o modelo *apontou* como sendo da \"Classe 1\", apenas 14% realmente eram. Ele está dando alguns \"alarmes falsos\".\n",
    "\n",
    "**Na prática, o que isso quer dizer?**\n",
    "\n",
    "Temos um modelo que é bom em **identificar um grupo que contém muitos dos clientes de risco**, mesmo que traga alguns clientes bons junto. Para o negócio, isso pode ser muito útil como um sistema de **alerta precoce**. Em vez de aprovar ou negar crédito automaticamente, o modelo poderia sinalizar os clientes que ele acha que são da Classe 1 para uma análise mais detalhada por um especialista.\n",
    "\n",
    "Em resumo, conseguimos construir, através de um processo sistemático de limpeza, seleção e otimização, um modelo com capacidade preditiva útil e com uma aplicação prática bem definida para o negócio."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
